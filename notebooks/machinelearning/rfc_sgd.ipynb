{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60fac90",
   "metadata": {},
   "source": [
    "# Pipeline RFC + SGD — Notebook productivo\n",
    "\n",
    "Este notebook implementa el pipeline de clasificación binaria para BTC usando dos modelos base: RandomForestClassifier (RFC) y SGDClassifier (SGD). El flujo incluye: carga y splits temporales; reentrenamiento con mejores hiperparámetros; calibración de probabilidades y búsqueda de umbral óptimo sin fuga; y dos fases de rolling mensual (2021–2024 y 2025) con embargo del día 1 y calibración en ventanas recientes. Los resultados se guardan en `data/processed/` y `data/out/`, y los modelos calibrados se versionan en `models/`. :contentReference[oaicite:0]{index=0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba2e28",
   "metadata": {},
   "source": [
    "## Bloque 0 — Imports y configuración de rutas\n",
    "\n",
    "En este bloque se importan las librerías, se fijan constantes globales (semilla, nombres de columnas, rangos temporales) y se definen rutas robustas para ejecutar el notebook desde la raíz del repo o desde `src/`. Además, se crean las carpetas necesarias si no existen. :contentReference[oaicite:1]{index=1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1703e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Bloque 0) Imports y configuración de rutas\n",
    "# ============================================================\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.calibration\")\n",
    "\n",
    "# Imports generales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Configuración de rutas\n",
    "from pathlib import Path\n",
    "\n",
    "# En notebook, __file__ no existe; usamos cwd(). Si ejecutas como script, usa resolve() sobre __file__.\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    # Notebook-friendly: si estás en /src, BASE_DIR es el padre; si ya estás en raíz, se mantiene.\n",
    "    cwd = Path.cwd().resolve()\n",
    "    BASE_DIR = cwd if (cwd / \"data\").exists() and (cwd / \"models\").exists() else cwd.parent\n",
    "\n",
    "# Subcarpetas principales\n",
    "DATA_DIR      = BASE_DIR / \"data\"\n",
    "RAW_DIR       = DATA_DIR / \"raw\"        # Datos originales descargados\n",
    "SRC_DIR       = DATA_DIR / \"src\"        # Datos intermedios (ej. 2025 con indicadores)\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"  # Datos enriquecidos (probas rolling 2021–2024/2025)\n",
    "OUT_DIR       = DATA_DIR / \"out\"        # Salidas finales listas para backtest/dashboards\n",
    "REPORTS_DIR   = BASE_DIR / \"reports\"    # Métricas, predicciones, análisis\n",
    "MODELS_DIR    = BASE_DIR / \"models\"     # Modelos entrenados y calibrados\n",
    "\n",
    "# Crear subcarpetas si no existen\n",
    "for d in [RAW_DIR, SRC_DIR, PROCESSED_DIR, OUT_DIR, REPORTS_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Archivos de entrada\n",
    "CSV_PATH       = SRC_DIR / \"btc_enriched_with_target.csv\"         # dataset histórico base (2015–2024)\n",
    "CSV_2025_SRC   = SRC_DIR / \"btc_yahoo_2025_whit_indicators.csv\"   # dataset 2025 con indicadores\n",
    "\n",
    "# Archivos procesados (pipeline)\n",
    "OOS_CSV_2021_2024 = PROCESSED_DIR / \"probas_oos_rfc_sgd_2021_2024.csv\"\n",
    "OOS_CSV_2025      = PROCESSED_DIR / \"probas_oos_rfc_sgd_2025.csv\"  # opcional\n",
    "\n",
    "# Archivo de salida enriquecido para 2025\n",
    "CSV_2025_WITH_PROBS = OUT_DIR / \"btc_2025_with_probs.csv\"\n",
    "\n",
    "# Paths opcionales de reports\n",
    "PRED_RFC_CSV    = REPORTS_DIR / \"predictions_RFC.csv\"\n",
    "METRICS_RFC_CSV = REPORTS_DIR / \"metrics_RFC.csv\"\n",
    "PRED_SGD_CSV    = REPORTS_DIR / \"predictions_SGD.csv\"\n",
    "METRICS_SGD_CSV = REPORTS_DIR / \"metrics_SGD.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88090c76",
   "metadata": {},
   "source": [
    "## Bloque 1 — Carga y splits (2015–2021)\n",
    "\n",
    "En este bloque se carga el dataset histórico enriquecido, se normalizan fechas y tipos, se filtran `NaN` generados por ventanas de indicadores y se separan variables (`X`) y objetivo (`y`). Luego se construyen los splits temporales TRAIN (2015–2020), VALID (2021-H1) y TEST (2021-H2), verificando que no haya solapamientos. :contentReference[oaicite:2]{index=2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea9b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cargando dataset histórico: D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\machine_learning\\data\\src\\btc_enriched_with_target.csv\n",
      ">> Columnas de entrada (X):\n",
      "['close', 'high', 'low', 'open', 'volume', 'RSI', 'MACD', 'MACD_SIGNAL', 'SMA20', 'EMA20', 'BB_UPPER', 'BB_LOWER', 'ATR', 'CCI']\n",
      "\n",
      ">> Resumen de splits:\n",
      "TRAIN (2015-2020): {'rango_fechas': (datetime.date(2015, 1, 20), datetime.date(2020, 12, 31)), 'muestras': 2173, 'features': 14, 'positivos_%': 55.177174413253574}\n",
      "VALID (2021-H1):   {'rango_fechas': (datetime.date(2021, 1, 1), datetime.date(2021, 6, 30)), 'muestras': 181, 'features': 14, 'positivos_%': 49.72375690607735}\n",
      "TEST  (2021-H2):   {'rango_fechas': (datetime.date(2021, 7, 1), datetime.date(2021, 12, 31)), 'muestras': 184, 'features': 14, 'positivos_%': 52.71739130434783}\n",
      "\n",
      ">> Verificación de solapamientos: OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario.tomatis\\AppData\\Local\\Temp\\ipykernel_31032\\1905928206.py:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Bloque 1) Carga y splits\n",
    "# ============================================================\n",
    "DATE_COL   = \"Date\"\n",
    "TARGET_COL = \"Target\"  # binaria 0/1\n",
    "\n",
    "# Rangos de tiempo (inclusivos) para train/val/test iniciales\n",
    "TRAIN_START, TRAIN_END = \"2015-01-01\", \"2020-12-31\"\n",
    "VAL_START,   VAL_END   = \"2021-01-01\", \"2021-06-30\"\n",
    "TEST_START,  TEST_END  = \"2021-07-01\", \"2021-12-31\"\n",
    "\n",
    "print(\">> Cargando dataset histórico:\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "assert DATE_COL in df.columns, f\"No se encontró la columna '{DATE_COL}'.\"\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], utc=False, errors=\"coerce\")\n",
    "df = df.sort_values(DATE_COL).dropna(subset=[DATE_COL]).reset_index(drop=True).set_index(DATE_COL)\n",
    "\n",
    "# Limpieza mínima\n",
    "df = df[~df.index.duplicated(keep=\"last\")]\n",
    "for c in df.columns:\n",
    "    if c != TARGET_COL:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "\n",
    "assert TARGET_COL in df.columns, f\"No se encontró '{TARGET_COL}'.\"\n",
    "if not set(df[TARGET_COL].dropna().unique()).issubset({0, 1}):\n",
    "    raise ValueError(f\"La columna '{TARGET_COL}' no es binaria 0/1.\")\n",
    "\n",
    "# Quitar filas con NaN (por ventanas de indicadores)\n",
    "df = df.dropna()\n",
    "\n",
    "# Features y target\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if TARGET_COL in num_cols:\n",
    "    num_cols.remove(TARGET_COL)\n",
    "\n",
    "X_all = df[num_cols].copy()\n",
    "y_all = df[TARGET_COL].astype(int).copy()\n",
    "\n",
    "# Splits temporales\n",
    "X_train = X_all.loc[TRAIN_START:TRAIN_END]\n",
    "y_train = y_all.loc[TRAIN_START:TRAIN_END]\n",
    "X_val   = X_all.loc[VAL_START:VAL_END]\n",
    "y_val   = y_all.loc[VAL_START:VAL_END]\n",
    "X_test  = X_all.loc[TEST_START:TEST_END]\n",
    "y_test  = y_all.loc[TEST_START:TEST_END]\n",
    "\n",
    "def _resumen_split(X, y):\n",
    "    return {\n",
    "        \"rango_fechas\": (X.index.min().date() if len(X) else None,\n",
    "                         X.index.max().date() if len(X) else None),\n",
    "        \"muestras\": len(X),\n",
    "        \"features\": X.shape[1],\n",
    "        \"positivos_%\": (float(y.mean())*100 if len(y) else np.nan)\n",
    "    }\n",
    "\n",
    "print(\">> Columnas de entrada (X):\")\n",
    "print(num_cols)\n",
    "print(\"\\n>> Resumen de splits:\")\n",
    "print(\"TRAIN (2015-2020):\", _resumen_split(X_train, y_train))\n",
    "print(\"VALID (2021-H1):  \", _resumen_split(X_val,   y_val))\n",
    "print(\"TEST  (2021-H2):  \", _resumen_split(X_test,  y_test))\n",
    "\n",
    "# Verificación de solapamientos\n",
    "assert len(X_train.index.intersection(X_val.index))  == 0, \"Solapamiento TRAIN-VALID.\"\n",
    "assert len(X_val.index.intersection(X_test.index))   == 0, \"Solapamiento VALID-TEST.\"\n",
    "assert len(X_train.index.intersection(X_test.index)) == 0, \"Solapamiento TRAIN-TEST.\"\n",
    "print(\"\\n>> Verificación de solapamientos: OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01828a7",
   "metadata": {},
   "source": [
    "## Bloque 2 — Reentrenar con mejores parámetros y evaluar en TEST (2021-H2)\n",
    "\n",
    "En este bloque se cargan los **mejores hiperparámetros** para RFC y SGD desde `reports/best_params.json`. Se reentrenan ambos modelos con `TRAIN+VALID` (2015–2021H1) y se evalúan en `TEST` (2021-H2), reportando métricas (Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC) y la matriz de confusión para un umbral 0.5. :contentReference[oaicite:3]{index=3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12cfb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Cargando parámetros desde best_params.json...\n",
      "Mejores params RFC: {'n_estimators': 400, 'max_depth': None, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "Mejores params SGD: {'alpha': 0.001, 'penalty': 'elasticnet', 'l1_ratio': 0.15, 'loss': 'log_loss', 'max_iter': 2000, 'tol': 0.001}\n",
      "\n",
      "=== RFC Test 2021-H2 (thr=0.5) ===\n",
      "Accuracy: 0.484 | Precision: 0.514 | Recall: 0.381 | F1: 0.438\n",
      "ROC AUC:  0.506 | PR AUC:   0.540\n",
      "Matriz de confusión:\n",
      "[[52 35]\n",
      " [60 37]]\n",
      "\n",
      "=== SGD Test 2021-H2 (thr=0.5) ===\n",
      "Accuracy: 0.473 | Precision: 0.000 | Recall: 0.000 | F1: 0.000\n",
      "ROC AUC:  0.556 | PR AUC:   0.600\n",
      "Matriz de confusión:\n",
      "[[87  0]\n",
      " [97  0]]\n",
      "\n",
      "--- Classification report (RFC) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.60      0.52        87\n",
      "           1       0.51      0.38      0.44        97\n",
      "\n",
      "    accuracy                           0.48       184\n",
      "   macro avg       0.49      0.49      0.48       184\n",
      "weighted avg       0.49      0.48      0.48       184\n",
      "\n",
      "\n",
      "--- Classification report (SGD) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64        87\n",
      "           1       0.00      0.00      0.00        97\n",
      "\n",
      "    accuracy                           0.47       184\n",
      "   macro avg       0.24      0.50      0.32       184\n",
      "weighted avg       0.22      0.47      0.30       184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Bloque 2) RFC y SGD: cargar parámetros desde JSON y evaluar\n",
    "# ============================================================\n",
    "def evaluar(nombre, y_true, y_proba, threshold=0.5):\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    pr  = average_precision_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else np.nan\n",
    "\n",
    "    print(f\"\\n=== {nombre} (thr=0.5) ===\")\n",
    "    print(f\"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")\n",
    "    print(f\"ROC AUC:  {roc:.3f} | PR AUC:   {pr:.3f}\")\n",
    "    print(\"Matriz de confusión:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"roc\": roc, \"pr\": pr}\n",
    "\n",
    "# 1) Cargar mejores parámetros desde JSON (obligatorio)\n",
    "params_path = REPORTS_DIR / \"best_params.json\"\n",
    "if not params_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No se encontró {params_path}. Genera primero ese archivo con los mejores parámetros.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n>> Cargando parámetros desde best_params.json...\")\n",
    "with open(params_path, \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_rfc_params = best_params[\"RFC\"]\n",
    "best_sgd_params = best_params[\"SGD\"]\n",
    "print(\"Mejores params RFC:\", best_rfc_params)\n",
    "print(\"Mejores params SGD:\", best_sgd_params)\n",
    "\n",
    "# 2) Reentrenar con TRAIN+VALID y evaluar en TEST (2021-H2)\n",
    "X_trval = pd.concat([X_train, X_val], axis=0).sort_index()\n",
    "y_trval = pd.concat([y_train, y_val], axis=0).sort_index()\n",
    "\n",
    "rfc_final = RandomForestClassifier(\n",
    "    **best_rfc_params,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfc_final.fit(X_trval, y_trval)\n",
    "rfc_test_proba = rfc_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "sgd_final = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", SGDClassifier(\n",
    "        **best_sgd_params,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "sgd_final.fit(X_trval, y_trval)\n",
    "if hasattr(sgd_final.named_steps[\"clf\"], \"predict_proba\"):\n",
    "    sgd_test_proba = sgd_final.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    sgd_scores = sgd_final.decision_function(X_test)\n",
    "    sgd_test_proba = 1 / (1 + np.exp(-sgd_scores))\n",
    "\n",
    "_ = evaluar(\"RFC Test 2021-H2\", y_test.values, rfc_test_proba, threshold=0.5)\n",
    "_ = evaluar(\"SGD Test 2021-H2\", y_test.values, sgd_test_proba, threshold=0.5)\n",
    "\n",
    "print(\"\\n--- Classification report (RFC) ---\")\n",
    "print(classification_report(y_test.values, (rfc_test_proba >= 0.5).astype(int), zero_division=0))\n",
    "print(\"\\n--- Classification report (SGD) ---\")\n",
    "print(classification_report(y_test.values, (sgd_test_proba >= 0.5).astype(int), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa438d",
   "metadata": {},
   "source": [
    "## Bloque 3 — Calibración sin fuga y búsqueda de umbral óptimo\n",
    "\n",
    "En este bloque se reentrena cada modelo con `TRAIN` (2015–2020), se **calibra** con `VALID` (2021-H1) evitando fuga de información (RFC con isotónica; SGD con Platt/sigmoid) y se **busca el umbral** que maximiza F1 en VALID. Con ese umbral y las probabilidades calibradas se evalúa en TEST (2021-H2). :contentReference[oaicite:4]{index=4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5248e21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> RFC — Umbral óptimo VALID: 0.2871 | F1_VALID=0.669\n",
      "\n",
      "=== RFC calibrado — TEST 2021-H2 (thr=0.287) ===\n",
      "Accuracy: 0.527 | Precision: 0.531 | Recall: 0.876 | F1: 0.661\n",
      "ROC AUC:  0.473 | PR AUC:   0.514\n",
      "Matriz de confusión:\n",
      "[[12 75]\n",
      " [12 85]]\n",
      ">> SGD — Umbral óptimo VALID: 0.4725 | F1_VALID=0.656\n",
      "\n",
      "=== SGD calibrado — TEST 2021-H2 (thr=0.473) ===\n",
      "Accuracy: 0.505 | Precision: 0.518 | Recall: 0.887 | F1: 0.654\n",
      "ROC AUC:  0.523 | PR AUC:   0.581\n",
      "Matriz de confusión:\n",
      "[[ 7 80]\n",
      " [11 86]]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Bloque 3) Calibración + Umbral óptimo (sin fuga)\n",
    "# ============================================================\n",
    "def evaluar_con_umbral(nombre, y_true, proba, thr):\n",
    "    y_pred = (proba >= thr).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1  = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc = roc_auc_score(y_true, proba) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    pr  = average_precision_score(y_true, proba) if len(np.unique(y_true)) > 1 else np.nan\n",
    "\n",
    "    print(f\"\\n=== {nombre} (thr={thr:.3f}) ===\")\n",
    "    print(f\"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")\n",
    "    print(f\"ROC AUC:  {roc:.3f} | PR AUC:   {pr:.3f}\")\n",
    "    print(\"Matriz de confusión:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    return dict(acc=acc, prec=prec, rec=rec, f1=f1, roc=roc, pr=pr, thr=thr)\n",
    "\n",
    "def buscar_umbral_f1(y_true, proba, n=200):\n",
    "    q05, q95 = np.quantile(proba, 0.05), np.quantile(proba, 0.95)\n",
    "    grid = np.linspace(q05, q95, n)\n",
    "    best_thr, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        f1 = f1_score(y_true, (proba >= t).astype(int), zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, t\n",
    "    return best_thr, best_f1\n",
    "\n",
    "# RFC: reentreno en TRAIN, calibración en VALID\n",
    "rfc_train = RandomForestClassifier(\n",
    "    **best_rfc_params,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfc_train.fit(X_train, y_train)\n",
    "rfc_cal = CalibratedClassifierCV(rfc_train, method=\"isotonic\", cv=\"prefit\")\n",
    "rfc_cal.fit(X_val, y_val)\n",
    "rfc_val_proba = rfc_cal.predict_proba(X_val)[:, 1]\n",
    "rfc_thr_opt, rfc_f1_val = buscar_umbral_f1(y_val.values, rfc_val_proba, n=200)\n",
    "print(f\">> RFC — Umbral óptimo VALID: {rfc_thr_opt:.4f} | F1_VALID={rfc_f1_val:.3f}\")\n",
    "rfc_test_proba_cal = rfc_cal.predict_proba(X_test)[:, 1]\n",
    "_ = evaluar_con_umbral(\"RFC calibrado — TEST 2021-H2\", y_test.values, rfc_test_proba_cal, rfc_thr_opt)\n",
    "\n",
    "# SGD: reentreno (pipeline) en TRAIN, calibración en VALID\n",
    "sgd_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"clf\", SGDClassifier(\n",
    "        **best_sgd_params,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "sgd_base.fit(X_train, y_train)\n",
    "sgd_cal = CalibratedClassifierCV(sgd_base, method=\"sigmoid\", cv=\"prefit\")\n",
    "sgd_cal.fit(X_val, y_val)\n",
    "sgd_val_proba = sgd_cal.predict_proba(X_val)[:, 1]\n",
    "sgd_thr_opt, sgd_f1_val = buscar_umbral_f1(y_val.values, sgd_val_proba, n=200)\n",
    "print(f\">> SGD — Umbral óptimo VALID: {sgd_thr_opt:.4f} | F1_VALID={sgd_f1_val:.3f}\")\n",
    "sgd_test_proba_cal = sgd_cal.predict_proba(X_test)[:, 1]\n",
    "_ = evaluar_con_umbral(\"SGD calibrado — TEST 2021-H2\", y_test.values, sgd_test_proba_cal, sgd_thr_opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39458a2c",
   "metadata": {},
   "source": [
    "## Bloque 4 — Rolling mensual 2021–2024 con embargo y calibración\n",
    "\n",
    "En este bloque se generan probabilidades **out-of-sample** para 2021–2024. Para cada mes `m`, se entrena en histórico previo, se calibra en una ventana reciente (60–90 días antes del corte) y se predicen los días del mes **excluyendo el día 1** (embargo). Se guardan `proba_rfc_cal`, `proba_sgd_cal`, `y_true` y `month` en `data/processed/probas_oos_rfc_sgd_2021_2024.csv`. :contentReference[oaicite:5]{index=5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b39894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Archivo generado (2021–2024): D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\machine_learning\\data\\processed\\probas_oos_rfc_sgd_2021_2024.csv\n",
      "            proba_rfc_cal  proba_sgd_cal  y_true    month\n",
      "date                                                     \n",
      "2021-01-02            0.6       0.604499       1  2021-01\n",
      "2021-01-03            0.6       0.561633       0  2021-01\n",
      "2021-01-04            0.6       0.533298       1  2021-01\n",
      "2021-01-05            0.6       0.570351       1  2021-01\n",
      "2021-01-06            0.6       0.570963       1  2021-01\n",
      "            proba_rfc_cal  proba_sgd_cal  y_true    month\n",
      "date                                                     \n",
      "2024-12-27       0.512821       0.566995       1  2024-12\n",
      "2024-12-28       0.512821       0.566896       0  2024-12\n",
      "2024-12-29       0.512821       0.567299       0  2024-12\n",
      "2024-12-30       0.512821       0.566694       1  2024-12\n",
      "2024-12-31       0.512821       0.566378       1  2024-12\n",
      "\n",
      "Resumen por año (2021–2024):\n",
      "      count      mean\n",
      "date                 \n",
      "2021    353  0.509915\n",
      "2022    353  0.473088\n",
      "2023    353  0.498584\n",
      "2024    354  0.525424\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Bloque 4) Rolling mensual 2021–2024 con embargo y calibración\n",
    "# ============================================================\n",
    "ROLL_START_2124 = pd.Timestamp(\"2021-01-01\")\n",
    "ROLL_END_2124   = pd.Timestamp(\"2024-12-31\")\n",
    "CAL_WINDOW_DAYS_MIN = 60\n",
    "CAL_WINDOW_DAYS_DEF = 90\n",
    "\n",
    "def month_end(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return ts + pd.offsets.MonthEnd(0)\n",
    "\n",
    "def month_start(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return ts + pd.offsets.MonthBegin(0)\n",
    "\n",
    "def fit_rfc_base_hist(X_hist, y_hist):\n",
    "    model = RandomForestClassifier(\n",
    "        **best_rfc_params,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_hist, y_hist)\n",
    "    return model\n",
    "\n",
    "def fit_sgd_base_hist(X_hist, y_hist):\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", SGDClassifier(\n",
    "            **best_sgd_params,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "    pipe.fit(X_hist, y_hist)\n",
    "    return pipe\n",
    "\n",
    "rows = []\n",
    "current = month_start(ROLL_START_2124)\n",
    "while current <= ROLL_END_2124:\n",
    "    m_start = month_start(current)\n",
    "    m_end   = month_end(current)\n",
    "    train_end = m_start - pd.Timedelta(days=1)\n",
    "\n",
    "    if train_end < X_all.index.min():\n",
    "        current = m_start + pd.offsets.MonthBegin(1)\n",
    "        continue\n",
    "\n",
    "    cal_end = train_end\n",
    "    cal_start = cal_end - pd.Timedelta(days=CAL_WINDOW_DAYS_DEF - 1)\n",
    "    if cal_start < X_all.index.min():\n",
    "        cal_start = X_all.index.min()\n",
    "    if (cal_end - cal_start).days + 1 < CAL_WINDOW_DAYS_MIN:\n",
    "        cal_start = cal_end - pd.Timedelta(days=CAL_WINDOW_DAYS_MIN - 1)\n",
    "        if cal_start < X_all.index.min():\n",
    "            cal_start = X_all.index.min()\n",
    "\n",
    "    train_base_end = cal_start - pd.Timedelta(days=1)\n",
    "    if train_base_end < X_all.index.min():\n",
    "        train_base_end = train_end\n",
    "\n",
    "    X_hist = X_all.loc[:train_base_end]\n",
    "    y_hist = y_all.loc[:train_base_end]\n",
    "    if len(X_hist) < 50:\n",
    "        X_hist = X_all.loc[:train_end]\n",
    "        y_hist = y_all.loc[:train_end]\n",
    "\n",
    "    X_cal = X_all.loc[cal_start:cal_end]\n",
    "    y_cal = y_all.loc[cal_start:cal_end]\n",
    "    if len(X_hist) < 50 or len(X_cal) < 30:\n",
    "        current = m_start + pd.offsets.MonthBegin(1)\n",
    "        continue\n",
    "\n",
    "    # Embargo: descartar el día 1 del mes (no se predice)\n",
    "    X_month_pred = X_all.loc[(m_start + pd.Timedelta(days=1)):m_end]\n",
    "    y_month_true = y_all.loc[(m_start + pd.Timedelta(days=1)):m_end]\n",
    "    if len(X_month_pred) == 0:\n",
    "        current = m_start + pd.offsets.MonthBegin(1)\n",
    "        continue\n",
    "\n",
    "    rfc_base = fit_rfc_base_hist(X_hist, y_hist)\n",
    "    rfc_cal_ = CalibratedClassifierCV(rfc_base, method=\"isotonic\", cv=\"prefit\")\n",
    "    rfc_cal_.fit(X_cal, y_cal)\n",
    "    rfc_proba_month = rfc_cal_.predict_proba(X_month_pred)[:, 1]\n",
    "\n",
    "    sgd_base_ = fit_sgd_base_hist(X_hist, y_hist)\n",
    "    sgd_cal_  = CalibratedClassifierCV(sgd_base_, method=\"sigmoid\", cv=\"prefit\")\n",
    "    sgd_cal_.fit(X_cal, y_cal)\n",
    "    sgd_proba_month = sgd_cal_.predict_proba(X_month_pred)[:, 1]\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        \"date\": X_month_pred.index,\n",
    "        \"proba_rfc_cal\": rfc_proba_month,\n",
    "        \"proba_sgd_cal\": sgd_proba_month,\n",
    "        \"y_true\": y_month_true.values,\n",
    "        \"month\": m_start.strftime(\"%Y-%m\")\n",
    "    }).set_index(\"date\")\n",
    "    rows.append(tmp)\n",
    "\n",
    "    current = m_start + pd.offsets.MonthBegin(1)\n",
    "\n",
    "oos_df = pd.concat(rows).sort_index()\n",
    "oos_df = oos_df.loc[\"2021-01-01\":\"2024-12-31\"]  # seguridad\n",
    "\n",
    "oos_df.to_csv(OOS_CSV_2021_2024, float_format=\"%.6f\")\n",
    "print(f\"\\nArchivo generado (2021–2024): {OOS_CSV_2021_2024}\")\n",
    "print(oos_df.head().to_string())\n",
    "print(oos_df.tail().to_string())\n",
    "print(\"\\nResumen por año (2021–2024):\")\n",
    "print(oos_df.groupby(oos_df.index.year)[\"y_true\"].agg(['count','mean']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f86a0",
   "metadata": {},
   "source": [
    "## Bloque 5 — Rolling mensual 2025 con embargo y calibración\n",
    "\n",
    "En este bloque se cargan los datos 2025 con indicadores, se alinean **exactamente** las columnas con las usadas en entrenamiento, y se ejecuta un rolling mensual de enero a agosto de 2025. Para cada mes, se entrena en histórico pre-2025, se calibra en una ventana reciente y se predicen los días del mes **desde el día 2** (embargo). Se guardan las probabilidades en `data/out/btc_2025_with_probs.csv` y se versionan los modelos calibrados por mes en `models/`. :contentReference[oaicite:6]{index=6}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b887c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cargando dataset 2025: D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\machine_learning\\data\\src\\btc_yahoo_2025_whit_indicators.csv\n",
      "[2025-01] OK — entrenado, calibrado y predicho 30 días. Modelos guardados.\n",
      "[2025-02] OK — entrenado, calibrado y predicho 27 días. Modelos guardados.\n",
      "[2025-03] OK — entrenado, calibrado y predicho 30 días. Modelos guardados.\n",
      "[2025-04] OK — entrenado, calibrado y predicho 29 días. Modelos guardados.\n",
      "[2025-05] OK — entrenado, calibrado y predicho 30 días. Modelos guardados.\n",
      "[2025-06] OK — entrenado, calibrado y predicho 29 días. Modelos guardados.\n",
      "[2025-07] OK — entrenado, calibrado y predicho 30 días. Modelos guardados.\n",
      "[2025-08] OK — entrenado, calibrado y predicho 29 días. Modelos guardados.\n",
      "\n",
      "Archivo con probabilidades 2025 guardado en: D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\machine_learning\\data\\out\\btc_2025_with_probs.csv\n",
      "  month  n_pred_days train_hist_end  cal_start    cal_end\n",
      "2025-01           30     2024-10-02 2024-10-03 2024-12-31\n",
      "2025-02           27     2024-11-02 2024-11-03 2025-01-31\n",
      "2025-03           30     2024-11-30 2024-12-01 2025-02-28\n",
      "2025-04           29     2024-12-31 2025-01-01 2025-03-31\n",
      "2025-05           30     2025-01-30 2025-01-31 2025-04-30\n",
      "2025-06           29     2025-03-02 2025-03-03 2025-05-31\n",
      "2025-07           30     2025-04-01 2025-04-02 2025-06-30\n",
      "2025-08           29     2025-05-02 2025-05-03 2025-07-31\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Bloque 5) Rolling mensual 2025 con embargo y calibración\n",
    "# ============================================================\n",
    "print(f\">> Cargando dataset 2025: {CSV_2025_SRC}\")\n",
    "df_2025_raw = pd.read_csv(CSV_2025_SRC)\n",
    "\n",
    "# === 1) Alinear nombres EXACTOS a los del entrenamiento ===\n",
    "train_cols = list(X_all.columns)  # features usadas en fit()\n",
    "lower_to_train = {c.lower(): c for c in train_cols}\n",
    "\n",
    "# detectar y parsear fecha\n",
    "date_col = \"Date\" if \"Date\" in df_2025_raw.columns else (\"date\" if \"date\" in df_2025_raw.columns else None)\n",
    "if date_col is None:\n",
    "    raise ValueError(\"El CSV 2025 no tiene columna 'Date'/'date'.\")\n",
    "\n",
    "# renombrar para que coincidan con entrenamiento (sin forzar minúsculas)\n",
    "ren = {c: (lower_to_train[c.lower()] if c.lower() in lower_to_train else c) for c in df_2025_raw.columns}\n",
    "df_2025 = df_2025_raw.rename(columns=ren).copy()\n",
    "\n",
    "# parseo fecha y orden\n",
    "df_2025[date_col] = pd.to_datetime(df_2025[date_col], utc=False, errors=\"coerce\")\n",
    "df_2025 = df_2025.dropna(subset=[date_col]).sort_values(date_col).set_index(date_col)\n",
    "\n",
    "# asegurar tipos numéricos de las features de entrenamiento\n",
    "for c in train_cols:\n",
    "    if c in df_2025.columns:\n",
    "        df_2025[c] = pd.to_numeric(df_2025[c], errors=\"coerce\")\n",
    "\n",
    "# verificar que estén las mismas features que vio el modelo\n",
    "missing = [c for c in train_cols if c not in df_2025.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Faltan columnas en 2025 que el modelo espera: {missing}\")\n",
    "\n",
    "# opcional: limpiar NaN SOLO de las features usadas\n",
    "df_2025 = df_2025.dropna(subset=train_cols)\n",
    "\n",
    "# === 2) DataFrame de salida y columnas de probabilidades ===\n",
    "df_out = df_2025.copy()\n",
    "for col in [\"proba_rfc_cal\", \"proba_sgd_cal\"]:\n",
    "    if col not in df_out.columns:\n",
    "        df_out[col] = np.nan\n",
    "\n",
    "# === 3) Parámetros del rolling 2025 ===\n",
    "ROLL_START = pd.Timestamp(\"2025-01-01\")\n",
    "ROLL_END   = pd.Timestamp(\"2025-08-31\")\n",
    "CAL_WINDOW_DAYS_DEF = 90\n",
    "CAL_WINDOW_DAYS_MIN = 60\n",
    "\n",
    "def fit_rfc_base(X, y):\n",
    "    model = RandomForestClassifier(\n",
    "        **best_rfc_params,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def fit_sgd_base(X, y):\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "        (\"clf\", SGDClassifier(\n",
    "            **best_sgd_params,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ])\n",
    "    pipe.fit(X, y)\n",
    "    return pipe\n",
    "\n",
    "# === 4) Rolling mensual (2025-01 a 2025-08) ===\n",
    "current = pd.Timestamp(ROLL_START)\n",
    "rows_info = []\n",
    "\n",
    "while current <= ROLL_END:\n",
    "    m_start = pd.Timestamp(current.replace(day=1))\n",
    "    m_end   = m_start + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    # Entrenamiento/calibración SOLO con histórico pre-2025\n",
    "    train_end = m_start - pd.Timedelta(days=1)\n",
    "    cal_end   = train_end\n",
    "    cal_start = cal_end - pd.Timedelta(days=CAL_WINDOW_DAYS_DEF - 1)\n",
    "\n",
    "    if cal_start < X_all.index.min():\n",
    "        cal_start = X_all.index.min()\n",
    "    if (cal_end - cal_start).days + 1 < CAL_WINDOW_DAYS_MIN:\n",
    "        cal_start = cal_end - pd.Timedelta(days=CAL_WINDOW_DAYS_MIN - 1)\n",
    "        if cal_start < X_all.index.min():\n",
    "            cal_start = X_all.index.min()\n",
    "\n",
    "    train_base_end = cal_start - pd.Timedelta(days=1)\n",
    "    if train_base_end < X_all.index.min():\n",
    "        train_base_end = train_end\n",
    "\n",
    "    X_hist = X_all.loc[:train_base_end]\n",
    "    y_hist = y_all.loc[:train_base_end]\n",
    "    X_cal  = X_all.loc[cal_start:cal_end]\n",
    "    y_cal  = y_all.loc[cal_start:cal_end]\n",
    "\n",
    "    if len(X_hist) < 50 or len(X_cal) < 30:\n",
    "        print(f\"[{m_start.strftime('%Y-%m')}] Insuficiente histórico/calibración. Mes omitido.\")\n",
    "        current = m_start + pd.offsets.MonthBegin(1)\n",
    "        continue\n",
    "\n",
    "    # Embargo: saltar el día 1\n",
    "    pred_start = m_start + pd.Timedelta(days=1)\n",
    "\n",
    "    # IMPORTANTE: usar EXACTAMENTE las mismas features y en el mismo orden\n",
    "    X_pred = df_out.loc[pred_start:m_end, train_cols]\n",
    "    if len(X_pred) == 0:\n",
    "        print(f\"[{m_start.strftime('%Y-%m')}] No hay días para predecir (tras embargo). Mes omitido.\")\n",
    "        current = m_start + pd.offsets.MonthBegin(1)\n",
    "        continue\n",
    "\n",
    "    # Entrenar + calibrar\n",
    "    rfc_base = fit_rfc_base(X_hist, y_hist)\n",
    "    rfc_cal  = CalibratedClassifierCV(rfc_base, method=\"isotonic\", cv=\"prefit\")\n",
    "    rfc_cal.fit(X_cal, y_cal)\n",
    "    rfc_proba = rfc_cal.predict_proba(X_pred)[:, 1]\n",
    "\n",
    "    sgd_base = fit_sgd_base(X_hist, y_hist)\n",
    "    sgd_cal  = CalibratedClassifierCV(sgd_base, method=\"sigmoid\", cv=\"prefit\")\n",
    "    sgd_cal.fit(X_cal, y_cal)\n",
    "    sgd_proba = sgd_cal.predict_proba(X_pred)[:, 1]\n",
    "\n",
    "    # Volcar probabilidades (solo días 2..fin)\n",
    "    df_out.loc[X_pred.index, \"proba_rfc_cal\"] = rfc_proba\n",
    "    df_out.loc[X_pred.index, \"proba_sgd_cal\"] = sgd_proba\n",
    "\n",
    "    model_tag = m_start.strftime('%Y-%m')\n",
    "    joblib.dump(rfc_cal, MODELS_DIR / f\"rfc_2025_{model_tag}.pkl\")\n",
    "    joblib.dump(sgd_cal, MODELS_DIR / f\"sgd_2025_{model_tag}.pkl\")\n",
    "\n",
    "    rows_info.append({\n",
    "        \"month\": model_tag,\n",
    "        \"n_pred_days\": int(len(X_pred)),\n",
    "        \"train_hist_end\": str(train_base_end.date()) if hasattr(train_base_end, \"date\") else str(train_base_end),\n",
    "        \"cal_start\": str(cal_start.date()) if hasattr(cal_start, \"date\") else str(cal_start),\n",
    "        \"cal_end\": str(cal_end.date()) if hasattr(cal_end, \"date\") else str(cal_end),\n",
    "    })\n",
    "    print(f\"[{model_tag}] OK — entrenado, calibrado y predicho {len(X_pred)} días. Modelos guardados.\")\n",
    "\n",
    "    current = m_start + pd.offsets.MonthBegin(1)\n",
    "\n",
    "# === 5) Guardar salida final ===\n",
    "df_out.to_csv(CSV_2025_WITH_PROBS, float_format=\"%.6f\")\n",
    "print(f\"\\nArchivo con probabilidades 2025 guardado en: {CSV_2025_WITH_PROBS}\")\n",
    "if rows_info:\n",
    "    print(pd.DataFrame(rows_info).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
