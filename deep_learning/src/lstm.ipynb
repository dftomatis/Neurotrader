{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e47c9ff",
   "metadata": {},
   "source": [
    "# Rolling mensual LSTM (BiLSTM + Atención) — Ventana 24 meses\n",
    "\n",
    "Este notebook implementa un **pipeline de predicción mensual** para BTC usando un **BiLSTM con atención**. Para cada **corte de fin de mes**:\n",
    "1. Construye una **ventana deslizante de 24 meses** (hasta el corte).\n",
    "2. Separa **train / validación tail** (~21 días finales).\n",
    "3. Realiza **fine-tuning** del modelo (early-stopping).\n",
    "4. **Calibra temperatura** (temperature scaling) en validación.\n",
    "5. **Predice el mes siguiente** y genera **señales** con **umbral fijo = 0.45**.\n",
    "6. Versiona artefactos en `models/rolling/YYYY-MM-DD/` y guarda predicciones en `data/out/rolling/`.\n",
    "\n",
    "## Entradas requeridas\n",
    "- CSV maestro (ej.: `data/src/btc_2022_2025.csv`) con columnas:\n",
    "  - `Date, open, high, low, close, volume`\n",
    "  - Indicadores: `RSI, MACD, MACD_SIGNAL, SMA20, EMA20, BB_UPPER, BB_LOWER, ATR, CCI`\n",
    "  - Probabilidades auxiliares: `proba_rfc_cal, proba_sgd_cal, proba_xgb_cal`\n",
    "  - Sentimiento: `proba_sentiment_neg, proba_sentiment_neu, proba_sentiment_pos`\n",
    "  - Etiqueta: `y_true` (1 si cierra arriba al día siguiente, 0 si no)\n",
    "\n",
    "- Artefactos base compatibles en `models/`:\n",
    "  - `lstm_config.json` (define `feature_cols`, `window_size`, arquitectura)\n",
    "  - `lstm_scaler.pkl`\n",
    "  - `lstm_weights.pth` (pesos iniciales)\n",
    "\n",
    "## Salidas\n",
    "- Por cada corte `YYYY-MM-DD`:\n",
    "  - `models/rolling/YYYY-MM-DD/{lstm_weights.pth, lstm_scaler.pkl, lstm_config.json, metrics.json, meta.json}`\n",
    "- Predicciones mensuales:\n",
    "  - `data/out/rolling/preds_YYYY-MM.csv` con: `Date, proba_lstm, y_true (si existe), signal`\n",
    "  - **Embargo del día 1** del mes: `proba_lstm = NaN`\n",
    "- Agregado anual:\n",
    "  - `data/out/rolling/preds_2025_full.csv`\n",
    "\n",
    "## Notas\n",
    "- **Umbral fijo** de señal: `0.45` sobre `proba_lstm`.\n",
    "- Validación **tail**: 21 días para early-stopping y calibración.\n",
    "- Se **congela la primera BiLSTM** para estabilidad durante el fine-tuning.\n",
    "- El escalado (`StandardScaler`) se ajusta **solo en train** de cada corte (evita fuga de información).\n",
    "\n",
    "## Ejecución\n",
    "- Ejecuta las celdas en orden. Asegúrate de que las rutas `data/` y `models/` existan y que los artefactos base sean consistentes con `feature_cols` y `window_size`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01a4d4",
   "metadata": {},
   "source": [
    "## 0. Imports, rutas y configuración base\n",
    "\n",
    "En este bloque cargamos todas las librerías necesarias para el pipeline, incluyendo `numpy`, `pandas`, `torch` y utilidades de scikit-learn.  \n",
    "Además, definimos rutas robustas que permiten ejecutar el notebook tanto desde la carpeta `/src` como desde la raíz del repositorio.  \n",
    "\n",
    "También configuramos los parámetros por defecto, entre ellos:\n",
    "\n",
    "- **Ubicación del CSV maestro** con los datos de 2022–2025.  \n",
    "- **Artefactos base del LSTM**: pesos iniciales, scaler y archivo de configuración.  \n",
    "- **Directorio de salida** para guardar predicciones y modelos entrenados de cada corte mensual.  \n",
    "\n",
    "Este bloque asegura que la estructura de carpetas y los artefactos previos estén correctamente inicializados antes de comenzar el pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ae9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Imports, rutas y configuración base\n",
    "# ============================================================\n",
    "import json, math, shutil, hashlib, argparse\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Rutas base (robustas desde /src o raíz del proyecto)\n",
    "THIS_DIR  = Path.cwd().resolve()\n",
    "BASE_DIR  = THIS_DIR if (THIS_DIR / \"data\").exists() and (THIS_DIR / \"models\").exists() else THIS_DIR.parent\n",
    "DATA_DIR  = BASE_DIR / \"data\"\n",
    "SRC_DIR   = DATA_DIR / \"src\"\n",
    "OUT_DIR   = DATA_DIR / \"out\"\n",
    "MODELS_DIR= BASE_DIR / \"models\"\n",
    "ROLLING_DIR = MODELS_DIR / \"rolling\"\n",
    "(OUT_DIR / \"rolling\").mkdir(parents=True, exist_ok=True)\n",
    "ROLLING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Entrada por defecto: CSV maestro con 2023, 2024 y 2025\n",
    "DEFAULT_INPUT = SRC_DIR / \"btc_2022_2025.csv\"  # ajusta si tu maestro se llama distinto\n",
    "\n",
    "# Artefactos base existentes (pesos entrenados hasta 2023, scaler y config compatibles)\n",
    "BASE_CFG  = MODELS_DIR / \"lstm_config.json\"\n",
    "BASE_SCL  = MODELS_DIR / \"lstm_scaler.pkl\"\n",
    "BASE_W    = MODELS_DIR / \"lstm_weights.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890e751",
   "metadata": {},
   "source": [
    "## 1. Parámetros del rolling y umbrales\n",
    "\n",
    "En este bloque definimos los parámetros principales para ejecutar el rolling mensual del modelo.  \n",
    "Se establece el rango de fechas de los cortes mensuales, el tamaño de la ventana de entrenamiento (24 meses) y el número de días destinados a validación (cola de aproximadamente 3 semanas).  \n",
    "\n",
    "También se configuran los hiperparámetros básicos de entrenamiento como número máximo de épocas, paciencia para early-stopping, tasa de aprendizaje, regularización y tamaño de batch.  \n",
    "\n",
    "Finalmente, se fija el umbral constante de 0.45 que se usará para convertir las probabilidades del modelo en señales de compra o venta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebea205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Parámetros del rolling y umbrales\n",
    "# ============================================================\n",
    "# Parámetros del rolling\n",
    "ROLL_START = pd.Timestamp(\"2024-12-31\")  # primer corte\n",
    "ROLL_END   = pd.Timestamp(\"2025-08-31\")  # último corte a generar (ajusta si tienes más datos)\n",
    "WINDOW_MONTHS = 24\n",
    "VAL_TAIL_DAYS = 21   # valida con ~3 semanas al final de la ventana\n",
    "\n",
    "# Entrenamiento\n",
    "MAX_EPOCHS    = 10\n",
    "PATIENCE      = 3\n",
    "LR            = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "BATCH_SIZE    = 256\n",
    "EMBARGO_DAY1  = True  # embargo día 1 en las predicciones del mes\n",
    "\n",
    "# Umbral fijo de señal\n",
    "THRESHOLD_FIXED = 0.45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3b851",
   "metadata": {},
   "source": [
    "## 2. Arquitectura: BiLSTM con Atención\n",
    "\n",
    "En este bloque se define el modelo secuencial utilizado en el pipeline.  \n",
    "La arquitectura proyecta primero las variables de entrada a un espacio latente mediante una capa lineal.  \n",
    "Luego se aplican dos capas LSTM bidireccionales, cada una seguida de normalización y dropout para mejorar la estabilidad del entrenamiento y reducir sobreajuste.  \n",
    "\n",
    "Sobre la salida de la segunda LSTM se aplica un mecanismo de atención, que asigna diferentes pesos a los pasos temporales de la secuencia y construye un vector de contexto.  \n",
    "Finalmente, este vector pasa por capas densas que producen la salida en forma de logits, que posteriormente se transforman en probabilidades mediante la función sigmoide.  \n",
    "\n",
    "El objetivo de esta arquitectura es capturar dependencias temporales largas y cortas en los datos, al mismo tiempo que destaca los días más relevantes dentro de cada ventana temporal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1603e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) Arquitectura: BiLSTM con Atención\n",
    "# ============================================================\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, n_features, d_model=64, h1=64, h2=32, attn_dim=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(n_features, d_model), nn.GELU(), nn.LayerNorm(d_model)\n",
    "        )\n",
    "        self.bilstm1 = nn.LSTM(d_model, h1, batch_first=True, bidirectional=True)\n",
    "        self.ln1 = nn.LayerNorm(2*h1); self.do1 = nn.Dropout(dropout)\n",
    "        self.bilstm2 = nn.LSTM(2*h1, h2, batch_first=True, bidirectional=True)\n",
    "        self.ln2 = nn.LayerNorm(2*h2); self.do2 = nn.Dropout(dropout)\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(2*h2, attn_dim), nn.Tanh(), nn.Linear(attn_dim, 1)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*h2, 64), nn.GELU(), nn.Dropout(dropout), nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x, _ = self.bilstm1(x); x = self.ln1(x); x = self.do1(x)\n",
    "        x, _ = self.bilstm2(x); x = self.ln2(x); x = self.do2(x)\n",
    "        scores  = self.attn(x).squeeze(-1)              # [B, T]\n",
    "        weights = torch.softmax(scores, dim=1).unsqueeze(-1)  # [B, T, 1]\n",
    "        context = (x * weights).sum(dim=1)              # [B, 2*h2]\n",
    "        return self.fc(context)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6e3da",
   "metadata": {},
   "source": [
    "## 3. Feature engineering\n",
    "\n",
    "En este bloque se generan variables adicionales a partir de las series originales de precios y volúmenes.  \n",
    "Se incluyen transformaciones logarítmicas del precio de cierre, retornos acumulados en diferentes horizontes, medidas de volatilidad, indicadores de momento y características de las velas japonesas (cuerpo relativo, mechas superior e inferior, rango alto-bajo).  \n",
    "\n",
    "También se incorporan transformaciones sobre el volumen y tasas de variación, así como medias exponenciales móviles de las probabilidades generadas por otros modelos (RFC, SGD y XGB).  \n",
    "\n",
    "Estas variables enriquecen el conjunto de datos con información estadística y técnica que facilita al modelo LSTM capturar patrones de mercado más complejos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26d4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Feature engineering\n",
    "# ============================================================\n",
    "def add_engineered_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_in.copy()\n",
    "    df[\"log_close\"] = np.log(df[\"close\"])\n",
    "    df[\"ret_1d\"] = df[\"log_close\"].diff()\n",
    "    df[\"ret_5d\"] = df[\"log_close\"].diff(5)\n",
    "    df[\"vol_7\"]  = df[\"ret_1d\"].rolling(7).std()\n",
    "    df[\"vol_14\"] = df[\"ret_1d\"].rolling(14).std()\n",
    "    df[\"mom_7\"]  = df[\"ret_1d\"].rolling(7).sum()\n",
    "    df[\"mom_14\"] = df[\"ret_1d\"].rolling(14).sum()\n",
    "    df[\"gap_open\"]   = np.log(df[\"open\"] / df[\"close\"].shift(1))\n",
    "    df[\"hl_range\"]   = (df[\"high\"] - df[\"low\"]) / df[\"close\"]\n",
    "    df[\"body_rel\"]   = (df[\"close\"] - df[\"open\"]) / df[\"open\"]\n",
    "    df[\"upper_wick\"] = (df[\"high\"] - df[[\"close\",\"open\"]].max(axis=1)) / df[\"open\"]\n",
    "    df[\"lower_wick\"] = (df[[\"close\",\"open\"]].min(axis=1) - df[\"low\"]) / df[\"open\"]\n",
    "    df[\"vol_log\"]    = np.log(df[\"volume\"].replace(0, np.nan))\n",
    "    df[\"vol_ret_1d\"] = df[\"vol_log\"].diff()\n",
    "\n",
    "    for p in [\"proba_rfc_cal\",\"proba_sgd_cal\",\"proba_xgb_cal\"]:\n",
    "        if p in df.columns:\n",
    "            df[f\"{p}_ema5\"]  = df[p].ewm(span=5,  adjust=False).mean()\n",
    "            df[f\"{p}_ema10\"] = df[p].ewm(span=10, adjust=False).mean()\n",
    "        else:\n",
    "            df[f\"{p}_ema5\"]  = np.nan\n",
    "            df[f\"{p}_ema10\"] = np.nan\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def month_first_mask(dates_np):\n",
    "    d = pd.to_datetime(dates_np)\n",
    "    try:\n",
    "        return (d.day == 1)\n",
    "    except Exception:\n",
    "        return (pd.Series(pd.to_datetime(dates_np)).dt.day == 1).to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b14487",
   "metadata": {},
   "source": [
    "## 4. Utilidades de datos y secuencias\n",
    "\n",
    "En este bloque se implementan funciones auxiliares para preparar los datos antes de entrenar el modelo.  \n",
    "Se incluyen rutinas para construir secuencias deslizantes de tamaño fijo a partir de las variables de entrada, lo que permite transformar la serie temporal en un conjunto de ventanas que alimentan al LSTM.  \n",
    "\n",
    "También se define una función para separar el conjunto de entrenamiento y validación, reservando los últimos días de la ventana como validación (val tail).  \n",
    "Finalmente, se incorpora una rutina de calibración de temperatura que ajusta la escala de los logits generados por el modelo, optimizando la correspondencia entre las probabilidades predichas y los valores reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974e5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Utilidades de datos y secuencias\n",
    "# ============================================================\n",
    "def make_sequences_from_frame(df_feat, feature_cols, window):\n",
    "    X = df_feat[feature_cols].values.astype(np.float32)\n",
    "    dates = df_feat[\"Date\"].values\n",
    "    X_seq, seq_dates = [], []\n",
    "    for i in range(window - 1, len(X)):\n",
    "        X_seq.append(X[i - window + 1 : i + 1])\n",
    "        seq_dates.append(dates[i])\n",
    "    X_seq = np.asarray(X_seq, dtype=np.float32)\n",
    "    seq_dates = pd.to_datetime(seq_dates)\n",
    "    return X_seq, seq_dates\n",
    "\n",
    "def split_train_val(df_feat, val_tail_days):\n",
    "    cutoff = df_feat[\"Date\"].max() - pd.Timedelta(days=val_tail_days)\n",
    "    tr = df_feat[df_feat[\"Date\"] <= cutoff].copy()\n",
    "    va = df_feat[df_feat[\"Date\"] >  cutoff].copy()\n",
    "    return tr, va\n",
    "\n",
    "def temperature_scale(logits, y, grid=np.linspace(0.5, 2.0, 16)):\n",
    "    best_T, best_metric = 1.0, -1.0\n",
    "    y = y.astype(int)\n",
    "    for T in grid:\n",
    "        proba = 1.0/(1.0 + np.exp(-logits / max(T,1e-4)))\n",
    "        try:\n",
    "            m = roc_auc_score(y, proba)\n",
    "        except Exception:\n",
    "            pred = (proba >= 0.5).astype(int)\n",
    "            m = f1_score(y, pred, zero_division=0)\n",
    "        if m > best_metric:\n",
    "            best_metric, best_T = m, T\n",
    "    return float(best_T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4125e85",
   "metadata": {},
   "source": [
    "## 5. Dataset PyTorch y entrenamiento\n",
    "\n",
    "En este bloque se define la clase Dataset que organiza las secuencias y etiquetas en el formato requerido por PyTorch.  \n",
    "Esto permite cargar los datos de forma eficiente mediante DataLoaders, facilitando el entrenamiento en lotes y la iteración por épocas.  \n",
    "\n",
    "Además, se implementa la rutina de entrenamiento del modelo.  \n",
    "Incluye la función de pérdida binaria con logits, el optimizador AdamW y un esquema de early-stopping basado en el desempeño en validación.  \n",
    "El criterio de validación principal es el AUC, aunque en caso de problemas se recurre al F1-score.  \n",
    "\n",
    "De esta manera, el modelo se ajusta de forma controlada, evitando sobreajuste y guardando los mejores pesos alcanzados en cada corte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771e579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Dataset PyTorch y entrenamiento\n",
    "# ============================================================\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_seq, y):\n",
    "        self.X = X_seq\n",
    "        self.y = y.reshape(-1,1).astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def train_one_cut(model, train_loader, val_loader, device, max_epochs=10, patience=3, lr=3e-4, wd=1e-4):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    opt = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    best_val = -1e9\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward(); opt.step()\n",
    "        # Validación: AUC\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_list, y_list = [], []\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                lg = model(xb)\n",
    "                logits_list.append(lg.cpu().numpy()); y_list.append(yb.cpu().numpy())\n",
    "            logits_val = np.vstack(logits_list).ravel()\n",
    "            y_val = np.vstack(y_list).ravel()\n",
    "            try:\n",
    "                auc = roc_auc_score(y_val, 1.0/(1.0+np.exp(-logits_val)))\n",
    "            except Exception:\n",
    "                pred = (logits_val >= 0).astype(int)\n",
    "                auc = f1_score(y_val, pred, zero_division=0)\n",
    "        if auc > best_val:\n",
    "            best_val = auc\n",
    "            best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=True)\n",
    "    return float(best_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f52a7e",
   "metadata": {},
   "source": [
    "## 6. Carga de configuración base e índice de cortes\n",
    "\n",
    "En este bloque se cargan los artefactos base del modelo LSTM: configuración inicial, scaler y pesos previamente entrenados.  \n",
    "Esto asegura la compatibilidad de las columnas de entrada y la arquitectura utilizada en el pipeline.  \n",
    "\n",
    "También se implementa la gestión del archivo `index.csv`, que actúa como registro central de cada corte mensual.  \n",
    "Allí se guardan las fechas de entrenamiento y validación, los hiperparámetros utilizados, el rendimiento en validación y las rutas de los artefactos generados.  \n",
    "\n",
    "De esta forma, se garantiza trazabilidad y versionado de los experimentos, permitiendo reproducir y auditar cada etapa del rolling mensual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de120358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) Carga de config base e índice de cortes\n",
    "# ============================================================\n",
    "def load_base_config():\n",
    "    if not BASE_CFG.exists(): raise FileNotFoundError(f\"No se encontró {BASE_CFG}\")\n",
    "    if not BASE_SCL.exists(): raise FileNotFoundError(f\"No se encontró {BASE_SCL}\")\n",
    "    if not BASE_W.exists():   raise FileNotFoundError(f\"No se encontró {BASE_W}\")\n",
    "    with open(BASE_CFG, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "    # asegurar feature_cols (compatibles con tus pesos)\n",
    "    feature_cols = cfg.get(\"feature_cols\")\n",
    "    if feature_cols is None:\n",
    "        feature_cols = cfg[\"tech_cols\"] + cfg[\"eng_cols\"] + cfg[\"proba_cols\"] + cfg[\"sent_cols\"]\n",
    "    window = int(cfg.get(\"window_size\", 5))\n",
    "    arch = dict(\n",
    "        d_model=int(cfg.get(\"d_model\", 64)),\n",
    "        h1=int(cfg.get(\"h1\", 64)),\n",
    "        h2=int(cfg.get(\"h2\", 32)),\n",
    "        attn_dim=int(cfg.get(\"attn_dim\", 32)),\n",
    "        dropout=float(cfg.get(\"dropout\", 0.3)),\n",
    "    )\n",
    "    temperature = float(cfg.get(\"temperature\", 1.0))\n",
    "    return cfg, feature_cols, window, arch, temperature\n",
    "\n",
    "def ensure_index_csv():\n",
    "    idx = ROLLING_DIR / \"index.csv\"\n",
    "    if not idx.exists():\n",
    "        pd.DataFrame(columns=[\n",
    "            \"cut_date\",\"train_start\",\"train_end\",\"val_start\",\"val_end\",\n",
    "            \"n_epochs\",\"val_metric\",\"best_threshold\",\"best_threshold_f1\",\n",
    "            \"weights_path\",\"scaler_path\",\"config_path\",\"notes\"\n",
    "        ]).to_csv(idx, index=False)\n",
    "    return idx\n",
    "\n",
    "def append_index_row(row: dict):\n",
    "    idx = ensure_index_csv()\n",
    "    df = pd.read_csv(idx)\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    df.to_csv(idx, index=False)\n",
    "\n",
    "def hash_series(s: pd.Series):\n",
    "    m = hashlib.md5()\n",
    "    m.update(pd.util.hash_pandas_object(s, index=False).values.tobytes())\n",
    "    return m.hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efda11e",
   "metadata": {},
   "source": [
    "## 7. Función principal del rolling mensual\n",
    "\n",
    "En este bloque se implementa el núcleo del pipeline.  \n",
    "Para cada corte mensual se construye una ventana de 24 meses y se divide en entrenamiento y validación.  \n",
    "Se ajusta un `StandardScaler` solo con los datos de entrenamiento para evitar fuga de información y se aplican los pesos iniciales adecuados (base o del corte anterior).  \n",
    "\n",
    "El modelo se entrena con early-stopping, se calibra la temperatura en validación y luego se versionan los artefactos generados (pesos, scaler, configuración, métricas y metadatos).  \n",
    "Además, se registran los resultados en el archivo `index.csv` para mantener un historial de cada corte.  \n",
    "\n",
    "Finalmente, se predice el mes siguiente aplicando un embargo en el primer día del mes y generando señales con el umbral fijo de 0.45.  \n",
    "Las predicciones se guardan en archivos mensuales y se consolida un CSV agregado con todos los resultados del periodo 2025.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96e4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) Función principal del rolling mensual\n",
    "# ============================================================\n",
    "def run_rolling():\n",
    "    # Cargar datos maestro\n",
    "    if not DEFAULT_INPUT.exists():\n",
    "        raise FileNotFoundError(f\"No existe el CSV maestro: {DEFAULT_INPUT}\")\n",
    "    df = pd.read_csv(DEFAULT_INPUT)\n",
    "    required = [\n",
    "        \"Date\",\"close\",\"high\",\"low\",\"open\",\"volume\",\n",
    "        \"RSI\",\"MACD\",\"MACD_SIGNAL\",\"SMA20\",\"EMA20\",\n",
    "        \"BB_UPPER\",\"BB_LOWER\",\"ATR\",\"CCI\",\n",
    "        \"proba_rfc_cal\",\"proba_sgd_cal\",\"proba_xgb_cal\",\n",
    "        \"proba_sentiment_neg\",\"proba_sentiment_neu\",\"proba_sentiment_pos\",\n",
    "        \"y_true\"\n",
    "    ]\n",
    "    miss = [c for c in required if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Faltan columnas requeridas: {miss}\")\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    # Feature engineering global (una sola vez)\n",
    "    df_feat_all = add_engineered_features(df)\n",
    "\n",
    "    # Config base y dispositivo\n",
    "    base_cfg, feature_cols, window, arch, base_temperature = load_base_config()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    preds_all = []\n",
    "    cut = ROLL_START\n",
    "\n",
    "    while cut <= ROLL_END:\n",
    "        # Ventana 24m hasta cut (incl.) y split train/val\n",
    "        start_win = (cut - pd.DateOffset(months=WINDOW_MONTHS)).normalize()\n",
    "        df_cut = df_feat_all[(df_feat_all[\"Date\"] >= start_win) & (df_feat_all[\"Date\"] <= cut)].copy()\n",
    "        if df_cut[\"Date\"].nunique() < (window + VAL_TAIL_DAYS + 10):\n",
    "            print(f\"[{cut.date()}] Ventana insuficiente, salto.\")\n",
    "            cut = (cut + pd.offsets.MonthEnd(1))\n",
    "            continue\n",
    "\n",
    "        needed = list(set(feature_cols + [\"Date\",\"close\",\"open\",\"high\",\"low\",\"volume\",\"y_true\"]))\n",
    "        df_cut = df_cut.dropna(subset=[c for c in needed if c in df_cut.columns]).copy()\n",
    "        for c in feature_cols:\n",
    "            df_cut[c] = pd.to_numeric(df_cut[c], errors=\"coerce\")\n",
    "        df_cut = df_cut.dropna(subset=feature_cols)\n",
    "\n",
    "        tr_df, va_df = split_train_val(df_cut, VAL_TAIL_DAYS)\n",
    "        if len(tr_df) < window*2 or len(va_df) < window+5:\n",
    "            print(f\"[{cut.date()}] Muy pocos datos para train/val, salto.\")\n",
    "            cut = (cut + pd.offsets.MonthEnd(1))\n",
    "            continue\n",
    "\n",
    "        # Scaler fit SOLO con train\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(tr_df[feature_cols].values.astype(np.float32))\n",
    "\n",
    "        # Estandarización vectorizada\n",
    "        tr_X_std = scaler.transform(tr_df[feature_cols].values.astype(np.float32))\n",
    "        va_X_std = scaler.transform(va_df[feature_cols].values.astype(np.float32))\n",
    "        tr_std = tr_df.copy(); tr_std[feature_cols] = tr_X_std\n",
    "        va_std = va_df.copy(); va_std[feature_cols] = va_X_std\n",
    "\n",
    "        Xtr, dtr = make_sequences_from_frame(tr_std, feature_cols, window)\n",
    "        Xva, dva = make_sequences_from_frame(va_std, feature_cols, window)\n",
    "        ytr = tr_std[\"y_true\"].values[window-1:].astype(np.float32)\n",
    "        yva = va_std[\"y_true\"].values[window-1:].astype(np.float32)\n",
    "\n",
    "        # Modelo\n",
    "        n_features = Xtr.shape[2]\n",
    "        model = BiLSTMWithAttention(n_features=n_features, **arch).to(device)\n",
    "\n",
    "        # Pesos iniciales: base en el primer corte; luego el del corte anterior\n",
    "        prev_cut_dir = (ROLLING_DIR / (cut - pd.offsets.MonthEnd(1)).strftime(\"%Y-%m-%d\"))\n",
    "        if cut == ROLL_START:\n",
    "            state = torch.load(BASE_W, map_location=device)\n",
    "        elif (prev_cut_dir / \"lstm_weights.pth\").exists():\n",
    "            state = torch.load(prev_cut_dir / \"lstm_weights.pth\", map_location=device)\n",
    "        else:\n",
    "            state = torch.load(BASE_W, map_location=device)\n",
    "        model.load_state_dict(state, strict=True)\n",
    "\n",
    "        # Congelar primera BiLSTM para estabilidad\n",
    "        for p in model.bilstm1.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # DataLoaders\n",
    "        tr_loader = torch.utils.data.DataLoader(SeqDataset(Xtr, ytr), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "        va_loader = torch.utils.data.DataLoader(SeqDataset(Xva, yva), batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Entrenamiento\n",
    "        best_val = train_one_cut(\n",
    "            model, tr_loader, va_loader, device,\n",
    "            max_epochs=MAX_EPOCHS, patience=PATIENCE, lr=LR, wd=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        # Calibración de temperatura (sobre validación)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_val = []\n",
    "            for xb, _ in va_loader:\n",
    "                logits_val.append(model(xb.to(device)).cpu().numpy())\n",
    "            logits_val = np.vstack(logits_val).ravel()\n",
    "        T_star = temperature_scale(logits_val, yva)\n",
    "\n",
    "        # Guardar artefactos del corte\n",
    "        cut_dir = ROLLING_DIR / cut.strftime(\"%Y-%m-%d\")\n",
    "        cut_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # scaler\n",
    "        joblib.dump(scaler, cut_dir / \"lstm_scaler.pkl\")\n",
    "        # pesos\n",
    "        torch.save(model.state_dict(), cut_dir / \"lstm_weights.pth\")\n",
    "        # config del corte (incluye umbral fijo)\n",
    "        cfg_cut = dict(\n",
    "            base_config_path=str(BASE_CFG),\n",
    "            feature_cols=feature_cols,\n",
    "            window_size=window,\n",
    "            d_model=arch[\"d_model\"], h1=arch[\"h1\"], h2=arch[\"h2\"],\n",
    "            attn_dim=arch[\"attn_dim\"], dropout=arch[\"dropout\"],\n",
    "            temperature=T_star,\n",
    "            threshold=THRESHOLD_FIXED,\n",
    "            threshold_mode=\"fixed\"\n",
    "        )\n",
    "        with open(cut_dir / \"lstm_config.json\", \"w\") as f:\n",
    "            json.dump(cfg_cut, f, indent=2)\n",
    "\n",
    "        # metrics y meta\n",
    "        metrics = dict(val_metric=best_val, threshold_used=THRESHOLD_FIXED, threshold_mode=\"fixed\")\n",
    "        with open(cut_dir / \"metrics.json\", \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "\n",
    "        meta = dict(\n",
    "            train_start=str(tr_df[\"Date\"].min().date()),\n",
    "            train_end=str(tr_df[\"Date\"].max().date()),\n",
    "            val_start=str(va_df[\"Date\"].min().date()),\n",
    "            val_end=str(va_df[\"Date\"].max().date()),\n",
    "            cut_date=str(cut.date()),\n",
    "            window_months=WINDOW_MONTHS,\n",
    "            val_tail_days=VAL_TAIL_DAYS,\n",
    "            lr=LR, weight_decay=WEIGHT_DECAY, batch_size=BATCH_SIZE,\n",
    "            max_epochs=MAX_EPOCHS, patience=PATIENCE\n",
    "        )\n",
    "        with open(cut_dir / \"meta.json\", \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        # index.csv\n",
    "        append_index_row(dict(\n",
    "            cut_date=str(cut.date()),\n",
    "            train_start=meta[\"train_start\"],\n",
    "            train_end=meta[\"train_end\"],\n",
    "            val_start=meta[\"val_start\"],\n",
    "            val_end=meta[\"val_end\"],\n",
    "            n_epochs=MAX_EPOCHS,\n",
    "            val_metric=best_val,\n",
    "            best_threshold=THRESHOLD_FIXED,\n",
    "            best_threshold_f1=np.nan,\n",
    "            weights_path=str((cut_dir / \"lstm_weights.pth\").relative_to(BASE_DIR)),\n",
    "            scaler_path=str((cut_dir / \"lstm_scaler.pkl\").relative_to(BASE_DIR)),\n",
    "            config_path=str((cut_dir / \"lstm_config.json\").relative_to(BASE_DIR)),\n",
    "            notes=\"threshold_mode=fixed\"\n",
    "        ))\n",
    "\n",
    "        # Predicción del mes siguiente\n",
    "        next_month_start = (cut + pd.offsets.Day(1)).normalize()\n",
    "        next_month_end   = (cut + pd.offsets.MonthEnd(1))\n",
    "        df_pred = df_feat_all[(df_feat_all[\"Date\"] >= next_month_start) & (df_feat_all[\"Date\"] <= next_month_end)].copy()\n",
    "        if len(df_pred) > 0:\n",
    "            # limpieza y std con scaler del corte\n",
    "            df_pred = df_pred.dropna(subset=[c for c in feature_cols + [\"Date\"] if c in df_pred.columns]).copy()\n",
    "            for c in feature_cols:\n",
    "                df_pred[c] = pd.to_numeric(df_pred[c], errors=\"coerce\")\n",
    "            df_pred = df_pred.dropna(subset=feature_cols)\n",
    "\n",
    "            Xp_std  = scaler.transform(df_pred[feature_cols].values.astype(np.float32))\n",
    "            df_pred_std = df_pred.copy(); df_pred_std[feature_cols] = Xp_std\n",
    "            Xp, dp = make_sequences_from_frame(df_pred_std, feature_cols, window)\n",
    "\n",
    "            if len(Xp) > 0:\n",
    "                with torch.no_grad():\n",
    "                    logits_p = []\n",
    "                    for i in range(0, len(Xp), 1024):\n",
    "                        xb = torch.from_numpy(Xp[i:i+1024]).to(device)\n",
    "                        logits_p.append(model(xb).cpu().numpy())\n",
    "                    logits_p = np.vstack(logits_p).ravel()\n",
    "                proba_p = 1.0/(1.0 + np.exp(-logits_p / max(T_star,1e-4)))\n",
    "                out = pd.DataFrame({\"Date\": dp, \"proba_lstm\": proba_p})\n",
    "                if \"y_true\" in df_pred.columns:\n",
    "                    out = out.merge(df_pred[[\"Date\",\"y_true\"]], on=\"Date\", how=\"left\")\n",
    "                # embargo día 1\n",
    "                if EMBARGO_DAY1:\n",
    "                    mask_first = month_first_mask(out[\"Date\"].values)\n",
    "                    out.loc[mask_first, \"proba_lstm\"] = np.nan\n",
    "                # señales con umbral FIJO\n",
    "                out[\"signal\"] = (out[\"proba_lstm\"] >= THRESHOLD_FIXED).astype(int)\n",
    "                # guardar mensuales\n",
    "                out_path = OUT_DIR / \"rolling\" / f\"preds_{next_month_start.strftime('%Y-%m')}.csv\"\n",
    "                out.sort_values(\"Date\").to_csv(out_path, index=False, float_format=\"%.6f\")\n",
    "                preds_all.append(out.assign(cut_date=cut.strftime(\"%Y-%m-%d\"),\n",
    "                                            thr_used=THRESHOLD_FIXED, T_used=T_star))\n",
    "                print(f\"[{cut.date()}] Guardado {out_path}, filas: {len(out)}\")\n",
    "\n",
    "        # siguiente corte\n",
    "        cut = (cut + pd.offsets.MonthEnd(1))\n",
    "\n",
    "    # Agregado final 2025\n",
    "    if preds_all:\n",
    "        full = pd.concat(preds_all, ignore_index=True).sort_values(\"Date\")\n",
    "        full.to_csv(OUT_DIR / \"rolling\" / \"preds_2025_full.csv\", index=False, float_format=\"%.6f\")\n",
    "        print(f\">> Guardado agregado: {OUT_DIR / 'rolling' / 'preds_2025_full.csv'}\")\n",
    "    else:\n",
    "        print(\"No se generaron predicciones mensuales.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f0b810",
   "metadata": {},
   "source": [
    "## 8. Ejecución del pipeline\n",
    "\n",
    "En este bloque se ejecuta la función principal que realiza todo el proceso de rolling mensual.  \n",
    "El pipeline utiliza como entrada el CSV maestro con los datos históricos y los artefactos base previamente entrenados.  \n",
    "\n",
    "Durante la ejecución se generan los modelos de cada corte, se guardan los artefactos en la carpeta `models/rolling/` y se producen los archivos de predicciones en `data/out/rolling/`.  \n",
    "Al finalizar, se obtiene además un archivo consolidado con todas las predicciones del año 2025.  \n",
    "\n",
    "Este paso permite reproducir el flujo completo de entrenamiento, validación, calibración y predicción de manera automática.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba2335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-31] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-01.csv, filas: 27\n",
      "[2025-01-31] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-02.csv, filas: 24\n",
      "[2025-02-28] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-03.csv, filas: 27\n",
      "[2025-03-31] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-04.csv, filas: 26\n",
      "[2025-04-30] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-05.csv, filas: 27\n",
      "[2025-05-31] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-06.csv, filas: 26\n",
      "[2025-06-30] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-07.csv, filas: 27\n",
      "[2025-07-31] Guardado D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025-08.csv, filas: 26\n",
      ">> Guardado agregado: D:\\Local\\Keepcoding\\Proyecto_Final\\git\\Neurotrader\\deep_learning\\data\\out\\rolling\\preds_2025_full.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8) Ejecutar pipeline\n",
    "# ============================================================\n",
    "_ = run_rolling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74a24f",
   "metadata": {},
   "source": [
    "## 9. Notas y resultados esperados\n",
    "\n",
    "En este bloque se resumen las entradas, salidas y consideraciones clave del pipeline.  \n",
    "\n",
    "**Entradas requeridas**  \n",
    "- CSV maestro con precios, indicadores técnicos, probabilidades auxiliares y sentimiento.  \n",
    "- Artefactos base del LSTM: configuración, scaler y pesos iniciales.  \n",
    "\n",
    "**Salidas generadas**  \n",
    "- Artefactos versionados por cada corte en `models/rolling/YYYY-MM-DD/`.  \n",
    "- Predicciones mensuales en `data/out/rolling/preds_YYYY-MM.csv`.  \n",
    "- Archivo consolidado con todas las predicciones del año en `data/out/rolling/preds_2025_full.csv`.  \n",
    "\n",
    "**Aspectos importantes**  \n",
    "- El umbral de decisión es fijo en 0.45 sobre la probabilidad generada por el LSTM.  \n",
    "- El primer día de cada mes queda embargado (probabilidad en NaN).  \n",
    "- La validación usa una cola de 21 días para early-stopping y calibración de temperatura.  \n",
    "- La primera capa BiLSTM se congela en cada corte para mejorar la estabilidad del ajuste fino.  \n",
    "\n",
    "Este resumen ayuda a verificar que el flujo completo produce los resultados esperados y que se mantiene la trazabilidad de cada etapa.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
